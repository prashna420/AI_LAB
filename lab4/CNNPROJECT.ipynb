{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ff0e750",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manage\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "#Utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "##plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "\n",
    "##ai_utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff565587",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "BATCH_SIZE = 128\n",
    "LR = 0.001\n",
    "WEIGHT_DECAY = 0.0001\n",
    "SEED = 42\n",
    "\n",
    "CLASSES = ['airplane','automobile','bird','cat','deer',\n",
    "           'dog','frog','horse','ship','truck']\n",
    "\n",
    "DATA_DIR    = './data'\n",
    "RESULTS_DIR = './results'\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "826f7957",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beafc785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version : 2.10.0+cpu\n",
      "Using CPU\n",
      "Device: cpu\n",
      "Cores: 16\n"
     ]
    }
   ],
   "source": [
    "print(f'PyTorch version : {torch.__version__}')\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "    print(f'Using GPU: {torch.cuda.get_device_name(0)}')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "    print('Using CPU')\n",
    "\n",
    "print(f'Device: {DEVICE}')\n",
    "print('Cores:', os.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e47eaeb",
   "metadata": {},
   "source": [
    "# DATA NORMALIZATION(MEAN,STANDARD DEVIATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075f2031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3.2%"
     ]
    }
   ],
   "source": [
    "raw_dataset = datasets.CIFAR10( root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "loader = DataLoader(raw_dataset, batch_size=1000, shuffle=False, num_workers=2)\n",
    "\n",
    "channel_sum    = torch.zeros(3)\n",
    "channel_sum_sq = torch.zeros(3)\n",
    "total_pixels   = 0\n",
    "\n",
    "for images, _ in loader:\n",
    "    batch_size = images.size(0)\n",
    "    images = images.view(batch_size, 3, -1)\n",
    "\n",
    "    channel_sum    += images.sum(dim=[0, 2])\n",
    "    channel_sum_sq += (images ** 2).sum(dim=[0, 2])\n",
    "    total_pixels   += batch_size * 32 * 32\n",
    "\n",
    "mean = channel_sum / total_pixels\n",
    "std  = torch.sqrt(channel_sum_sq / total_pixels - mean ** 2)\n",
    "\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Std:  {std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbf8e89",
   "metadata": {},
   "source": [
    "# AUGMENTING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08409bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTS\n",
    "MEAN = mean\n",
    "STD = std\n",
    "\n",
    "\n",
    "# Data Augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(MEAN, STD),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(MEAN, STD),\n",
    "])\n",
    "\n",
    "# Using provided test train set\n",
    "train_dataset = datasets.CIFAR10(root=DATA_DIR, train=True,  download=True, transform=train_transform)\n",
    "test_dataset  = datasets.CIFAR10(root=DATA_DIR, train=False, download=True, transform=test_transform)\n",
    "\n",
    "# Appying the Augmentation\n",
    "train_loader  = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,  num_workers=2, pin_memory=True)\n",
    "test_loader   = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f'Train: {len(train_dataset):,} samples | Test: {len(test_dataset):,} samples')\n",
    "print(f'Train batches: {len(train_loader)} | Test batches: {len(test_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05400bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img_tensor, title=None):\n",
    "    img = img_tensor.numpy().transpose((1, 2, 0)) # changing into poytorh format\n",
    "    img = np.clip(img * np.array(STD) + np.array(MEAN), 0, 1)\n",
    "    plt.imshow(img)\n",
    "    if title: plt.title(title, fontsize=8)\n",
    "    plt.axis('off')\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "fig, axes = plt.subplots(2, 6, figsize=(16, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    plt.sca(ax)\n",
    "    imshow(images[i], CLASSES[labels[i]])\n",
    "plt.suptitle('Sample CIFAR-10 Training Images (with augmentation)', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_DIR}/sample_images.png', dpi=120, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5e1f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.4):\n",
    "        \"\"\"\n",
    "        We are using 3*32*32 = 3072 features as input\n",
    "        The output is 10 classes\n",
    "\n",
    "        In simple NN we loose the spatial information since we flatten the image\n",
    "        The BatchNormal is used to reduce computation time using the current batch\n",
    "         - std dev - mean\n",
    "        Droupout is used to prevent overfitting by cutting off impact of some neurons\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(), # Flattening of spatial data occours here\n",
    "\n",
    "            nn.Linear(3*32*32, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "\n",
    "            nn.Linear(256, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1232051",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNetCIFAR(nn.Module):\n",
    "    def __init__(self, num_classes=10, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Here We implement the AlexNet architecture for CIFAR-10 dataset\n",
    "        We are using 3*32*32 2d images as input\n",
    "\n",
    "        There are 8 layers in total\n",
    "        There are 5 layers of convolution and 3 layers of fully connected\n",
    "        2 max pool layer within the 5 Convolutional layers to reduce the size of the data\n",
    "        This architecture is a copy of the AlexNet architecture as provided in the resource\n",
    "\n",
    "\n",
    "        The output is 10 classes\n",
    "        \"\"\"\n",
    "\n",
    "        # Convolutional Layer\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 192, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            nn.Conv2d(192, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((4, 4))\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256 * 4 * 4, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight); nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01); nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b7d30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic Block for CNN\n",
    "    Helper class for TinyVGG\n",
    "    used instead of repeating the same code multiple times\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, kernel=3, pad=1):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=kernel, padding=pad),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x): return self.block(x)\n",
    "\n",
    "\n",
    "class TinyVGG(nn.Module):\n",
    "    def __init__(self, num_classes=10, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Tiny VGG implementation\n",
    "        \"\"\"\n",
    "\n",
    "        self.block1 = nn.Sequential(\n",
    "            ConvBlock(3, 32), ConvBlock(32, 32),\n",
    "            nn.MaxPool2d(2, 2),#32 to 16\n",
    "            nn.Dropout2d(dropout_rate),\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            ConvBlock(32, 64), ConvBlock(64, 64),\n",
    "            nn.MaxPool2d(2, 2),# 16 to 8\n",
    "            nn.Dropout2d(dropout_rate),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 8 * 8, 256),# hence here 64x8x8\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "        self._init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight); nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight); nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7b9b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "models = {\n",
    "    'SimpleNN': SimpleNN(dropout_rate=0.4),\n",
    "    'AlexNet':  AlexNetCIFAR(num_classes=10, dropout_rate=0.5),\n",
    "    'TinyVGG':  TinyVGG(num_classes=10, dropout_rate=0.3),\n",
    "}\n",
    "\n",
    "print('Model Parameter Counts:')\n",
    "print('-' * 40)\n",
    "for name, model in models.items():\n",
    "    params = count_parameters(model)\n",
    "    print(f'  {name:10s}: {params:>10,} parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e99046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion, device, scaler=None):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if scaler:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                out  = model(images)\n",
    "                loss = criterion(out, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            out  = model(images)\n",
    "            loss = criterion(out, labels)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        correct    += out.argmax(1).eq(labels).sum().item()\n",
    "        total      += images.size(0)\n",
    "\n",
    "    return total_loss/total, 100.0*correct/total\n",
    "\n",
    "\n",
    "def eval_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            out  = model(images)\n",
    "            loss = criterion(out, labels)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            correct    += out.argmax(1).eq(labels).sum().item()\n",
    "            total      += images.size(0)\n",
    "\n",
    "    return total_loss/total, 100.0*correct/total\n",
    "\n",
    "\n",
    "def train_model(model, name, train_loader, test_loader,\n",
    "                device, epochs=EPOCHS, lr=LR, wd=WEIGHT_DECAY):\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n",
    "    scaler    = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n",
    "\n",
    "    history = {'train_loss':[], 'train_acc':[], 'test_loss':[], 'test_acc':[]}\n",
    "    best_acc = 0.0\n",
    "\n",
    "    print(f'\\n{\"=\"*55}')\n",
    "    ##print(f'  Training: {name}  ({count_parameters(model):,} params)')\n",
    "    print(f'{\"=\"*55}')\n",
    "\n",
    "    start = time.time()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, criterion, device, scaler)\n",
    "        te_loss, te_acc = eval_epoch(model, test_loader, criterion, device)\n",
    "        scheduler.step()\n",
    "\n",
    "        history['train_loss'].append(tr_loss)\n",
    "        history['train_acc'].append(tr_acc)\n",
    "        history['test_loss'].append(te_loss)\n",
    "        history['test_acc'].append(te_acc)\n",
    "\n",
    "        if te_acc > best_acc:\n",
    "            best_acc = te_acc\n",
    "            torch.save(model.state_dict(), f'{name.lower()}_best.pth')\n",
    "\n",
    "        if epoch % 5 == 0 or epoch == 1:\n",
    "            print(f'  Epoch [{epoch:3d}/{epochs}] '\n",
    "                  f'Train Loss: {tr_loss:.4f} Acc: {tr_acc:.2f}% | '\n",
    "                  f'Test Loss: {te_loss:.4f} Acc: {te_acc:.2f}%')\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    history['total_time'] = elapsed\n",
    "    history['best_acc']   = best_acc\n",
    "    print(f'\\n  ✓ Best Test Acc: {best_acc:.2f}%  |  Time: {elapsed:.1f}s ({elapsed/60:.1f} min)')\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73381194",
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = {}\n",
    "for name, model in models.items():\n",
    "    histories[name] = train_model(\n",
    "        model, name, train_loader, test_loader, DEVICE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db44ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_metrics(model, loader, device, class_names=CLASSES):\n",
    "    model.eval()\n",
    "    all_preds, all_targets, all_outputs = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, lbls in loader:\n",
    "            imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "            out  = model(imgs)\n",
    "            all_outputs.append(out.cpu())\n",
    "            all_preds.extend(out.argmax(1).cpu().numpy())\n",
    "            all_targets.extend(lbls.cpu().numpy())\n",
    "\n",
    "    outputs_tensor = torch.cat(all_outputs)\n",
    "    preds   = np.array(all_preds)\n",
    "    targets = np.array(all_targets)\n",
    "\n",
    "    return {\n",
    "        'accuracy':  accuracy_score(targets, preds) * 100,\n",
    "        'precision': precision_score(targets, preds, average='macro', zero_division=0) * 100,\n",
    "        'recall':    recall_score(targets, preds, average='macro', zero_division=0) * 100,\n",
    "        'f1':        f1_score(targets, preds, average='macro', zero_division=0) * 100,\n",
    "        'cm':        confusion_matrix(targets, preds),\n",
    "        'report':    classification_report(targets, preds, target_names=class_names, zero_division=0),\n",
    "        'preds':     preds,\n",
    "        'targets':   targets,\n",
    "    }\n",
    "\n",
    "all_metrics = {}\n",
    "print('Computing test set metrics...')\n",
    "for name, model in models.items():\n",
    "    all_metrics[name] = compute_all_metrics(model, test_loader, DEVICE)\n",
    "    m = all_metrics[name]\n",
    "    print(f'\\n  {name}: Acc={m[\"accuracy\"]:.2f}%  P={m[\"precision\"]:.2f}%  '\n",
    "          f'R={m[\"recall\"]:.2f}%  F1={m[\"f1\"]:.2f}% ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addbf733",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, m in all_metrics.items():\n",
    "    print(f'\\n{\"-\"*55}')\n",
    "    print(f'  Classification Report — {name}')\n",
    "    print(f'{\"-\"*55}')\n",
    "    print(m['report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71cf2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(21, 7))\n",
    "\n",
    "for ax, (name, m) in zip(axes, all_metrics.items()):\n",
    "    cm_pct = m['cm'].astype(float) / m['cm'].sum(axis=1, keepdims=True) * 100\n",
    "    sns.heatmap(cm_pct, ax=ax, annot=True, fmt='.1f', cmap='Blues',\n",
    "                xticklabels=CLASSES, yticklabels=CLASSES,\n",
    "                cbar_kws={'label': '% of True Class'}, linewidths=0.4)\n",
    "    ax.set_title(f'{name}\\nAcc: {m[\"accuracy\"]:.2f}%', fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('Predicted'); ax.set_ylabel('True')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.suptitle('Confusion Matrices — % of True Class', fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_DIR}/confusion_matrices.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7c2e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for name in all_metrics:\n",
    "    m = all_metrics[name]\n",
    "    h = histories[name]\n",
    "    rows.append({\n",
    "        'Model':          name,\n",
    "        'Accuracy (%)':   round(m['accuracy'],  2),\n",
    "        'Precision (%)':  round(m['precision'], 2),\n",
    "        'Recall (%)':     round(m['recall'],    2),\n",
    "        'F1 (%)':         round(m['f1'],        2),\n",
    "        'Params (K)':     round(count_parameters(models[name]) / 1000, 1),\n",
    "        'Train Time (s)': round(h['total_time'], 1),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows).set_index('Model')\n",
    "print('\\n' + '='*75)\n",
    "print('  FINAL COMPARISON TABLE')\n",
    "print('='*75)\n",
    "print(df.to_string())\n",
    "print('='*75)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7655d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_keys   = ['accuracy', 'precision', 'recall', 'f1', 'top5_acc']\n",
    "metric_labels = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'Top-5 Acc']\n",
    "names  = list(all_metrics.keys())\n",
    "x      = np.arange(len(metric_keys))\n",
    "width  = 0.25\n",
    "colors_list = ['red', 'blue', 'green']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "for i, (name, color) in enumerate(zip(names, colors_list)):\n",
    "    vals = [all_metrics[name][k] for k in metric_keys]\n",
    "    bars = ax.bar(x + i*width, vals, width, label=name, color=color, alpha=0.85, edgecolor='white')\n",
    "    for bar, v in zip(bars, vals):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.4,\n",
    "                f'{v:.1f}%', ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
    "\n",
    "ax.set_xticks(x + width); ax.set_xticklabels(metric_labels, fontsize=11)\n",
    "ax.set_ylabel('Score (%)'); ax.set_ylim(0, 115)\n",
    "ax.set_title('Model Comparison: All Evaluation Metrics', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11); ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_DIR}/metric_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_python (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
